import re
import json
import time
import requests
from bs4 import BeautifulSoup

# üîπ URL de la page de recherche Centris (liste de plex √† Montr√©al)
CENTRIS_SEARCH_URL = "https://www.centris.ca/fr/plex~a-vendre~montreal-ile?uc=0"

# üîπ URL de ton analyseur d√©j√† d√©ploy√© sur Render
ANALYZER_URL = "https://centris-analyse-bot.onrender.com/analyze"

# üîπ Fichier local pour m√©moriser les annonces d√©j√† analys√©es
SEEN_FILE = "seen_listings.json"


def load_seen_ids():
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except FileNotFoundError:
        return set()


def save_seen_ids(ids_set):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(sorted(list(ids_set)), f, ensure_ascii=False, indent=2)


def extract_listing_id(url: str):
    """
    Trouve un ID Centris √† 7 ou 8 chiffres dans l'URL de fiche.
    Exemple: .../19122184
    """
    m = re.search(r"/(\d{7,8})(?:[^\d]|$)", url)
    if m:
        return m.group(1)
    return None


def get_listing_urls_from_search():
    print(f"T√©l√©chargement de {CENTRIS_SEARCH_URL}")
    resp = requests.get(
        CENTRIS_SEARCH_URL,
        headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"},
        timeout=20,
    )
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    urls = []

    # On r√©cup√®re tous les liens vers des propri√©t√©s
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/fr/propriete" in href or "/fr/maison" in href or "/fr/plex" in href:
            if href.startswith("http"):
                full_url = href
            else:
                full_url = "https://www.centris.ca" + href
            urls.append(full_url)

    # On enl√®ve les doublons en gardant l'ordre
    unique = list(dict.fromkeys(urls))
    print(f"{len(unique)} URLs trouv√©es (avant filtrage par ID).")
    return unique


def analyze_listing(url: str):
    print(f"Analyse de {url}")
    resp = requests.post(
        ANALYZER_URL,
        json={"content": url},
        headers={"Content-Type": "application/json"},
        timeout=60,
    )

    if resp.status_code != 200:
        print(f"  ‚ùå Erreur HTTP {resp.status_code} : {resp.text[:200]}")
        return None

    try:
        data = resp.json()
    except Exception:
        print("  ‚ùå R√©ponse non JSON :")
        print(resp.text[:500])
        return None

    print("  ‚úÖ Analyse OK.")
    return data


def main():
    seen = load_seen_ids()
    print(f"{len(seen)} annonces d√©j√† analys√©es.\n")

    urls = get_listing_urls_from_search()

    for url in urls:
        listing_id = extract_listing_id(url)

        # üî∏ Si pas d'ID (pas de 7-8 chiffres dans l'URL), on ignore
        if not listing_id:
            print(f"üî∏ Pas d'ID dans l'URL, on ignore : {url}")
            continue

        # üî∏ Si d√©j√† vue, on saute
        if listing_id in seen:
            print(f"‚è© Annonce {listing_id} d√©j√† vue, on saute.")
            continue

        # üîπ Analyse de la fiche d√©taill√©e
        data = analyze_listing(url)

        # üîπ Si OK, on ajoute √† la liste des vues
        if data:
            seen.add(listing_id)
            save_seen_ids(seen)

        # Petite pause pour ne pas spammer Centris
        time.sleep(2)


if __name__ == "__main__":
    main()
