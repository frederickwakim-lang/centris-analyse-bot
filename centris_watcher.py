import re
import json
import time
import os
import requests
from bs4 import BeautifulSoup

# üîπ URL de la recherche Centris
CENTRIS_SEARCH_URL = os.getenv(
    "CENTRIS_SEARCH_URL",
    "https://www.centris.ca/fr/plex~a-vendre?uc=5",
)

# üîπ URL de ton analyseur sur Render
ANALYZER_URL = os.getenv(
    "ANALYZER_URL",
    "https://centris-analyse-bot.onrender.com/analyze",
)

# üîπ Fichier pour stocker les IDs d√©j√† analys√©s
SEEN_FILE = "seen_listings.json"


def load_seen_ids():
    """Charge la liste des IDs d√©j√† analys√©s depuis le fichier JSON local."""
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except FileNotFoundError:
        return set()


def save_seen_ids(ids_set):
    """Sauvegarde la liste des IDs d√©j√† analys√©s dans le fichier JSON local."""
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(sorted(list(ids_set)), f, ensure_ascii=False, indent=2)


def extract_listing_id(url: str):
    """Extrait l'ID Centris (7‚Äì8 chiffres) d'une URL."""
    m = re.search(r"/(\d{7,8})(?:[^\d]|$)", url)
    return m.group(1) if m else None


def get_listing_urls_from_search():
    """T√©l√©charge la page de recherche et r√©cup√®re toutes les URLs de fiches."""
    print(f"üîé T√©l√©chargement : {CENTRIS_SEARCH_URL}")
    resp = requests.get(
        CENTRIS_SEARCH_URL,
        headers={"User-Agent": "Mozilla/5.0"},
        timeout=20,
    )
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    urls = []

    # On r√©cup√®re toutes les URLs de fiches
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if any(x in href for x in ["/fr/duplex", "/fr/triplex", "/fr/quadruplex", "/fr/plex"]):
            if href.startswith("http"):
                full_url = href
            else:
                full_url = "https://www.centris.ca" + href
            urls.append(full_url)

    unique = list(dict.fromkeys(urls))
    print(f"‚û°Ô∏è {len(unique)} URLs trouv√©es")
    return unique


def analyze_listing(url: str):
    """Envoie l'URL d'une fiche √† ton analyseur Render et retourne le JSON analys√©."""
    print(f"üß† Analyse : {url}")
    try:
        resp = requests.post(
            ANALYZER_URL,
            json={"url": url},
            headers={"Content-Type": "application/json"},
            timeout=45,  # ‚è± max 45 secondes pour √©viter de rester bloqu√© trop longtemps
        )
    except requests.Timeout:
        print("  ‚ùå Timeout vers l'analyseur (trop long), on saute cette annonce.")
        return None
    except Exception as e:
        print(f"  ‚ùå Erreur r√©seau : {e}")
        return None

    if resp.status_code != 200:
        print(f"  ‚ùå Erreur HTTP {resp.status_code} : {resp.text[:300]}")
        return None

    try:
        data = resp.json()
        print("  ‚úÖ Analyse OK")
        return data
    except Exception:
        print("  ‚ùå R√©ponse non JSON :", resp.text[:500])
        return None


def main():
    seen = load_seen_ids()
    print(f"üìÇ {len(seen)} annonces d√©j√† analys√©es.\n")

    urls = get_listing_urls_from_search()

    for url in urls:
        listing_id = extract_listing_id(url)

        if not listing_id:
            print(f"üî∏ Pas d'ID : {url}")
            continue

        if listing_id in seen:
            print(f"‚è© D√©j√† vue : {listing_id}")
            continue

        data = analyze_listing(url)

        if data:
            seen.add(listing_id)
            save_seen_ids(seen)

        # Petite pause pour ne pas spammer Centris ni ton analyseur
        time.sleep(2)


if __name__ == "__main__":
    main()
